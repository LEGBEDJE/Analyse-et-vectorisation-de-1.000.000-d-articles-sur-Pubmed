{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59a1db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/legbedje/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import Entrez, Medline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Téléchargement des stopwords pour le traitement\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = \"votre-email@example.com\"  # Remplacez par votre email\n",
    "Entrez.api_key = \"votre-clé-api\"          # Remplacez par votre clé API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6eb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer des articles en lots avec gestion des erreurs\n",
    "def fetch_articles_batch(id_batch, retries=3):\n",
    "    ids_str = \",\".join(id_batch)\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"pubmed\", id=ids_str, rettype=\"medline\", retmode=\"text\")\n",
    "            records = Medline.parse(handle)\n",
    "            records_list = list(records)\n",
    "            handle.close()\n",
    "            return records_list\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur pour le lot, tentative {attempt + 1}/{retries}: {e}\")\n",
    "            time.sleep(1)  # Pause avant une nouvelle tentative\n",
    "    print(f\"Échec après {retries} tentatives pour le lot : {id_batch}\")\n",
    "    return []\n",
    "\n",
    "# Paralléliser la récupération des articles\n",
    "def fetch_pubmed_articles(pubmed_ids, batch_size=100, max_workers=5):\n",
    "    articles = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(fetch_articles_batch, pubmed_ids[i:i + batch_size])\n",
    "                   for i in range(0, len(pubmed_ids), batch_size)]\n",
    "        for future in futures:\n",
    "            articles.extend(future.result())\n",
    "            time.sleep(0.1)  # Pause légère pour éviter la surcharge API\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e76a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage des textes\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Mise en minuscule\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Suppression des références\n",
    "    text = re.sub(r'\\W', ' ', text)  # Suppression des caractères spéciaux\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Suppression des espaces multiples\n",
    "    text = text.strip()  # Suppression des espaces en début/fin\n",
    "    return text\n",
    "\n",
    "# Nettoyer les abstracts\n",
    "def preprocess_abstracts(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['cleaned_abstract'] = df['Abstract'].apply(lambda x: ' '.join(\n",
    "        [word for word in clean_text(x).split() if word not in stop_words]))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_statistics(df):\n",
    "    # Ajouter des colonnes pour analyse\n",
    "    df['abstract_length'] = df['Abstract'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "    df['num_authors'] = df['Authors'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "\n",
    "    # Statistiques descriptives\n",
    "    desc_stats = df[['abstract_length', 'num_authors']].describe()\n",
    "    print(\"Statistiques descriptives :\\n\", desc_stats)\n",
    "\n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['abstract_length'], bins=50, kde=True)\n",
    "    plt.title(\"Distribution de la longueur des résumés\")\n",
    "    plt.xlabel(\"Nombre de mots\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['num_authors'], bins=50, kde=False)\n",
    "    plt.title(\"Distribution du nombre d'auteurs par article\")\n",
    "    plt.xlabel(\"Nombre d'auteurs\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1411c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_cluster(df):\n",
    "    # Vectorisation avec TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_abstract'])\n",
    "\n",
    "    # Réduction de dimensionnalité avec PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    tfidf_reduced = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "    # Clustering avec K-means\n",
    "    kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "    clusters = kmeans.fit_predict(tfidf_reduced)\n",
    "\n",
    "    # Ajouter les clusters au DataFrame\n",
    "    df['cluster'] = clusters\n",
    "\n",
    "    # Visualisation des clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=tfidf_reduced[:, 0], y=tfidf_reduced[:, 1], hue=df['cluster'], palette='viridis', alpha=0.7)\n",
    "    plt.title(\"Visualisation des clusters des articles (PCA)\")\n",
    "    plt.xlabel(\"Composante principale 1\")\n",
    "    plt.ylabel(\"Composante principale 2\")\n",
    "    plt.show()\n",
    "    \n",
    "    return tfidf_matrix, clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd269efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Charger les IDs PubMed (exemple depuis un fichier CSV)\n",
    "    pubmed_ids_csv = pd.read_csv(\"pubmed_ids.csv\")  # Remplacez par votre fichier\n",
    "    pubmed_ids = pubmed_ids_csv['pubmedid'].astype(str).tolist()\n",
    "\n",
    "    # 2. Récupérer les articles\n",
    "    start_time = time.time()\n",
    "    pubmed_data = fetch_pubmed_articles(pubmed_ids, batch_size=100, max_workers=5)\n",
    "    end_time = time.time()\n",
    "    print(f\"Extraction terminée en {end_time - start_time:.2f} secondes\")\n",
    "\n",
    "    # 3. Extraire les champs pertinents\n",
    "    extracted_data = []\n",
    "    for record in pubmed_data:\n",
    "        pmid = record.get('PMID', '')\n",
    "        title = record.get('TI', '')\n",
    "        abstract = record.get('AB', '')\n",
    "        date = record.get('DP', '')\n",
    "        authors = \", \".join(record.get('AU', []))\n",
    "        \n",
    "        extracted_data.append({\n",
    "            'PMID': pmid,\n",
    "            'Title': title,\n",
    "            'Abstract': abstract,\n",
    "            'Date': date,\n",
    "            'Authors': authors\n",
    "        })\n",
    "\n",
    "    # 4. Créer un DataFrame\n",
    "    df = pd.DataFrame(extracted_data)\n",
    "\n",
    "    # 5. Nettoyer les données\n",
    "    df = preprocess_abstracts(df)\n",
    "\n",
    "    # 6. Analyse exploratoire\n",
    "    descriptive_statistics(df)\n",
    "\n",
    "    # 7. Vectorisation et clustering\n",
    "    tfidf_matrix, clusters = vectorize_and_cluster(df)\n",
    "\n",
    "    # 8. Sauvegarder les résultats\n",
    "    df.to_csv(\"pubmed_clustered_data.csv\", index=False)# fichier lourd je ne peut pas le commit\n",
    "    print(\"Données sauvegardées dans pubmed_clustered_data.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
